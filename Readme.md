## Image Captioning

### Overview:
In this project, the objective was to generate captions for images using a model built on an encoder-decoder architecture. The encoder component of the model leverages the Torchvision Resnet101 pretrained model, while the decoder portion is implemented using LSTM.

#### Dataset:
The model was trained using the Flicker8k Dataset

#### Validation and inference:
The trained model demonstrated the capability to provide reasonable descriptions, achieving a test score of 32.32.

### Demo
Below are a few sample inferences generated by the trained model:

#### References
> [1] https://github.com/yurayli/image-caption-pytorch/tree/b517cddcd2e11fdb017faa1eb67a7990558b4040
> [2] Show and Tell: A Neural Image Caption Generator (https://arxiv.org/abs/1411.4555)